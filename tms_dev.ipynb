{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a62206f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import unittest\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import csv\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import json\n",
    "import sys\n",
    "from datetime import datetime\n",
    "import pickle\n",
    "from collections import Counter\n",
    "from pandarallel import pandarallel\n",
    "import re\n",
    "import math\n",
    "import mysql.connector\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b6a8b21",
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################################################$$##########\n",
    "# Reading information from json file. Used to extract the parameters from the `config.json`.\n",
    "def read_json(path:str = \"config.json\") -> dict:\n",
    "    \"\"\"\n",
    "    path : str -> path of the json file\n",
    "    \"\"\"\n",
    "\n",
    "    with open('config.json') as config:\n",
    "        config_f = json.load(config)\n",
    "\n",
    "    return config_f\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "458e6c35",
   "metadata": {},
   "outputs": [],
   "source": [
    "#####################################################\n",
    "# Creating folder according to the and program scheme\n",
    "def create_folders(req_folders : list = [\"temp_data\", \"tms_input\", \"reports\"]):\n",
    "    \"\"\"\n",
    "    req_folders : str -> required folders path, if subfolder exsits input '\\\\' between folders.\n",
    "    \"\"\"\n",
    "\n",
    "    for folder in req_folders:\n",
    "        if os.path.exists(folder) is False:\n",
    "            os.mkdir(folder)\n",
    "            print(f\"> folder `{folder}` was created.\")\n",
    "\n",
    "        else:\n",
    "            print(f\"> folder `{folder}` exists, continuing.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a679d45d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#####################\n",
    "# sql connector class\n",
    "class mysql_connector():\n",
    "\n",
    "    # Getting the connection information from the config\n",
    "    def __init__(self):\n",
    "        self.conn_cred = read_json()[\"sql\"]\n",
    "\n",
    "    # Setting the sql connection\n",
    "    def setup_conn(self):\n",
    "        try:\n",
    "            self.sql_conn = mysql.connector.connect(\n",
    "                                                    host=self.conn_cred[\"adress\"],\n",
    "                                                    user=self.conn_cred[\"username\"],\n",
    "                                                    passwd=self.conn_cred[\"password\"],\n",
    "                                                    auth_plugin='mysql_native_password',\n",
    "                                                    )\n",
    "            print(\"> Established connection to the MySQL server.\")\n",
    "            \n",
    "            return self.sql_conn\n",
    "\n",
    "        except:\n",
    "            raise Exception(\"> Failed to establish connection to the MySQL server!\")\n",
    "        \n",
    "    # Closing the sql connection\n",
    "    def close_conn(self):\n",
    "        self.sql_conn.close()\n",
    "        print(\"> Connection to the MySQL was closed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46e36391",
   "metadata": {},
   "outputs": [],
   "source": [
    "######################################################\n",
    "# A small helper to send output to both CMD and a file\n",
    "class OutputTee:\n",
    "    def __init__(self, *streams):\n",
    "        self.streams = streams\n",
    "\n",
    "    def write(self, data):\n",
    "        for stream in self.streams:\n",
    "            stream.write(data)\n",
    "            stream.flush()\n",
    "\n",
    "    def flush(self):\n",
    "        for stream in self.streams:\n",
    "            stream.flush()\n",
    "\n",
    "\n",
    "# ###################\n",
    "# Running Pipeline with test\n",
    "def run_pipeline(test_pipeline,\n",
    "                 pipeline_name:str = \"\"\n",
    "                 ):\n",
    "    \"\"\"\n",
    "    test_pipeline -> the pipeline uninitited unittest pipeline we want to run\n",
    "    pipeline_name : str -> pipeline name in string format.\n",
    "    \"\"\"\n",
    "    current_time = datetime.now().strftime(\"%Y-%m-%d-%H-%M\")\n",
    "    db = read_json()[\"database\"][\"db_name\"]\n",
    "    reports_path = f\"reports\\\\{db}\\\\\"\n",
    "    create_folders([reports_path])\n",
    "    \n",
    "\n",
    "    # f is your text file -> sys.stdout is the CMD consol\n",
    "    with open(reports_path+f\"{pipeline_name}_[{current_time}]_report_.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "        # sys.stdout is the CMD console\n",
    "        # f is your text file\n",
    "        dual_stream = OutputTee(sys.stdout, f)\n",
    "        \n",
    "        runner = unittest.TextTestRunner(\n",
    "            stream=dual_stream, \n",
    "            verbosity=2, \n",
    "            descriptions=True\n",
    "        )\n",
    "\n",
    "        # Initialize the runner\n",
    "        runner = unittest.TextTestRunner(\n",
    "                stream=dual_stream, \n",
    "                verbosity=2, \n",
    "                descriptions=True\n",
    "                )\n",
    "        \n",
    "        # unitest  loader object\n",
    "        loader = unittest.TestLoader()\n",
    "\n",
    "        # Load tests from the specific class\n",
    "        suite = loader.loadTestsFromTestCase(test_pipeline) \n",
    "        \n",
    "        # Run with high verbosity for detail\n",
    "        result = runner.run(suite)\n",
    "        \n",
    "        # Custom detailed summary\n",
    "        print(\"\\n--- PIPELINE EXECUTION SUMMARY ---\")\n",
    "        if result.wasSuccessful():\n",
    "            print(\"Final Status: SUCCESS V\")\n",
    "        else:\n",
    "            print(f\"Final Status: FAILED X ({len(result.failures) + len(result.errors)} issues found)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ae0a6f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from source.helpers import protein\n",
    "\n",
    "class PipelinePreprocessingTest(unittest.TestCase):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "\n",
    "    ##################\n",
    "    # Class initiation \n",
    "    @classmethod\n",
    "    def setUpClass(cls):\n",
    "        print(\"--- Initializing Preprocessing Pipeline Environment ---\")\n",
    "\n",
    "        # Importing config information\n",
    "        cls.config_sql = read_json()[\"sql\"]\n",
    "        cls.config_db = read_json()[\"database\"]\n",
    "        cls.db_name = cls.config_db[\"db_name\"]\n",
    "        cls.time_start = datetime.now()\n",
    "        cls.subjects = list(cls.config_db[\"subject_id\"])\n",
    "        \n",
    "\n",
    "        # Cheecking if database assosiated talbes exsists\n",
    "        create_folders()\n",
    "        create_folders(req_folders=[f\"{i}//{cls.db_name}\" for i in [\"temp_data\", \"tms_input\", \"reports\"]])\n",
    "\n",
    "        # Setting paths\n",
    "        cls.path_temp = f\"temp_data//{cls.db_name}//\"\n",
    "        cls.path_final = f\"tms_input//{cls.db_name}//\"\n",
    "        cls.trimer_dict_path = \"source\\\\tables\\\\trimersDict.csv\"\n",
    "\n",
    "\n",
    "    ################################################################################################\n",
    "    # Cheecking if the the step already processed the data for this step\n",
    "    # First step of the preprocessing pipeline, extracting data from the sql server For each subject\n",
    "    def test_01_translate_convert_extract(self):\n",
    "\n",
    "        # Itirating over the subjects (list from `json.config`)\n",
    "        for subject_i in self.subjects:\n",
    "            toFile = self.path_temp + \"1_{}_{}.csv\".format(self.db_name, subject_i)\n",
    "            toFile1 = self.path_temp + \"1_AA_{}_{}.csv\".format(self.db_name, subject_i)\n",
    "            toFile2 = self.path_temp + \"1_{}_{}_seqK.csv\".format(self.db_name, subject_i)\n",
    "\n",
    "            bool_files = [os.path.exists(i) for i in [toFile, toFile1, toFile2]]\n",
    "\n",
    "            if sum(bool_files) == 3:\n",
    "                print(\"> Step No.1 of the preprocessing already done, continuing to step 2.\")\n",
    "            \n",
    "            else:\n",
    "                # Setting up sql connection\n",
    "                mysql_conn = mysql_connector()\n",
    "                mydb = mysql_conn.setup_conn()\n",
    "                \n",
    "                \n",
    "                #mydb = mysql.connector.connect(\n",
    "                #                                host=self.config_sql[\"adress\"],\n",
    "                #                                user=self.config_sql[\"username\"],\n",
    "                #                                passwd=self.config_sql[\"password\"],\n",
    "                #                                auth_plugin='mysql_native_password',\n",
    "                #                               )\n",
    "\n",
    "                cmd = f\"\"\"SELECT seq.*, coll.* FROM {self.db_name}.sequences \n",
    "                                                                           AS seq INNER JOIN {self.db_name}.sequence_collapse \n",
    "                                                                           AS coll ON seq.ai=coll.seq_ai WHERE seq.subject_id={int(subject_i)} \n",
    "                            AND seq.functional=1 AND coll.instances_in_subject !=0 \n",
    "                            AND coll.copy_number_in_subject > 1\n",
    "                            AND seq.deletions is null \n",
    "                            AND seq.insertions is null\"\"\"\n",
    "                \n",
    "                mycursor = mydb.cursor(dictionary=True)\n",
    "\n",
    "                removed = f\"temp_data\\\\{self.db_name}rem.csv\"\n",
    "\n",
    "                #Command for getting the sequences translated:\n",
    "                command = (cmd)\n",
    "                print(\"Translating starts ....\")     \n",
    "                mycursor.execute(command)\n",
    "                seq = mycursor.fetchall()\n",
    "                with open(toFile, 'w',newline='') as new_file:\n",
    "                    csv_writer = csv.writer(new_file)\n",
    "                    csv_writer.writerow(['seq_id','sequence','TranslatedSeq','TranslatedGermline','ai','subject_id','clone_id','sample_id'])\n",
    "                    for line in tqdm(seq):\n",
    "                            dna=\"\"\n",
    "                            germ=\"\"\n",
    "                            protein_sequence=\"\"\n",
    "                            #fix the germline to match the cdr with N's\n",
    "                            germ=line['germline']\n",
    "                            cdr3Length=line['cdr3_num_nts']\n",
    "                            postCDR=line['post_cdr3_length']\n",
    "                            x=int(cdr3Length)+int(postCDR)\n",
    "                            replaced=germ[-x:]\n",
    "                            replaced=replaced.replace('-','N',x) \n",
    "                            germ=germ.replace(germ[-x:],replaced)\n",
    "                            # -------------- To NNNNNNNNNNNNNN #\n",
    "                            dna=line['sequence']\n",
    "                            seqID=line['seq_id']\n",
    "                            ai =line['ai']\n",
    "                            cloneID=line['clone_id']\n",
    "                            sampleID=line['sample_id']\n",
    "                            subjectID=line['subject_id']\n",
    "                            # Generate protein sequence\n",
    "                            for i in range(0, len(dna)-(len(dna)%3), 3):\n",
    "                                if dna[i] == \"N\" and dna[i+1] == \"N\" and dna[i+2] == \"N\":\n",
    "                                    protein_sequence += \"x\"\n",
    "                                elif dna[i] == \"N\" or dna[i+1] == \"N\" or dna[i+2] == \"N\" and dna[i] != \"-\" and dna[i+1] != \"-\" and dna[i+2] != \"-\":\n",
    "                                    protein_sequence += \"x\"\n",
    "                                elif dna[i] == \"-\" and dna[i+1] == \"-\" and dna[i+2] == \"-\":\n",
    "                                    protein_sequence += protein[dna[i:i+3]]\n",
    "                                elif dna[i] == \"-\" or dna[i+1] == \"-\" or dna[i+2] == \"-\":\n",
    "                                    i=i+0;\n",
    "                                else:\n",
    "                                    protein_sequence += protein[dna[i:i+3]]\n",
    "                            germProtein_sequence=\"\"\n",
    "                            for i in range(0, len(germ)-(len(germ)%3), 3):\n",
    "                                if germ[i] == \"N\" and germ[i+1] == \"N\" and germ[i+2] == \"N\":\n",
    "                                    germProtein_sequence += \"x\"\n",
    "                                elif germ[i] == \"N\" or germ[i+1] == \"N\" or germ[i+2] == \"N\" and germ[i] != \"-\" and germ[i+1] != \"-\" and germ[i+2] != \"-\":\n",
    "                                    germProtein_sequence += \"x\"\n",
    "                                elif germ[i] == \"-\" and germ[i+1] == \"-\" and germ[i+2] == \"-\":\n",
    "                                    germProtein_sequence += protein[germ[i:i+3]]\n",
    "                                elif germ[i] == \"-\" or germ[i+1] == \"-\" or germ[i+2] == \"-\":\n",
    "                                    i=i+0;\n",
    "                                else:\n",
    "                                    germProtein_sequence += protein[germ[i:i+3]]\n",
    "                            csv_writer.writerow([seqID,dna,protein_sequence,germProtein_sequence,ai,subjectID,cloneID,sampleID])\n",
    "                    print(\"Tranlating DONE!\")\n",
    "                \n",
    "                #Matrix for the mutations\n",
    "                def build_matrix(rows, cols):\n",
    "                    matrix = []\n",
    "                    for r in range(0, rows):\n",
    "                        matrix.append([0 for c in range(0, cols)])\n",
    "                    return matrix\n",
    "                #Mutation function\n",
    "                def mutatedFunc(seqAA,germAA):\n",
    "                    global flag\n",
    "                    flag=0\n",
    "                    vec=build_matrix(2, len(seqAA))\n",
    "                    if len(seqAA)!=len(germAA):\n",
    "                        csv_writer1.writerow([seqAA,germAA])\n",
    "                        flag=1\n",
    "                    else:\n",
    "                        for i in range(0,len(seqAA),1):\n",
    "                            vec[0][i]=i+1\n",
    "                        # print(seqAA[i],germAA[i])\n",
    "                            if seqAA[i]!=germAA[i] and seqAA[i]!= \"x\" and seqAA[i]!=\"-\" and germAA[i]!= \"x\" and germAA[i]!=\"-\" and seqAA[i]!=\"*\" and germAA[i]!=\"*\":\n",
    "                                vec[1][i]=1\n",
    "                    return vec\n",
    "                \n",
    "                print(\"AA-mutations starts ....\")     \n",
    "                with open(toFile,'r') as csv_file:\n",
    "                    csv_reader = csv.DictReader(csv_file)\n",
    "                    with open(toFile1, 'w',newline='') as new_file ,open(removed, 'w',newline='') as nfile:\n",
    "                        csv_writer = csv.writer(new_file)\n",
    "                        csv_writer.writerow(['ai','sequence','seq_id','translatedSeq','translatedGerm','vector','subject_id','clone_id','sample_id'])\n",
    "                        csv_writer1 = csv.writer(nfile)\n",
    "                        csv_writer1.writerow(['translatedSeq','translatedGerm'])\n",
    "\n",
    "                        for line in (csv_reader):\n",
    "                            seq=(line['sequence'])\n",
    "                            seqID=(line['seq_id'])\n",
    "                            ai =(line['ai'])\n",
    "                            seqAA=(line['TranslatedSeq'])\n",
    "                            germAA=(line['TranslatedGermline'])\n",
    "                            cloneID=(line['clone_id'])\n",
    "                            sampleID=(line['sample_id'])\n",
    "                            subjectID=(line['subject_id'])\n",
    "                            vec1=mutatedFunc(seqAA, germAA)\n",
    "                            vector=[]\n",
    "                            if flag != 1: \n",
    "                                for i in range(len(vec1[0])):\n",
    "                                    if vec1[1][i]==1:\n",
    "                                        vector.append(i+1)\n",
    "                                csv_writer.writerow([ai,seq,seqID,seqAA,germAA,vector,subjectID,cloneID,sampleID])\n",
    "                    print(\"AA-mutations DONE!\")     \n",
    "                \n",
    "                def kmersFunc(AA,k):\n",
    "                    global start\n",
    "                    start=0\n",
    "                    p=0\n",
    "                    kmer=\"\"\n",
    "                    x=1\n",
    "                    s=1\n",
    "                    while(x!=20):\n",
    "                        if AA[-s] != \"-\":\n",
    "                            x+=1\n",
    "                            s+=1\n",
    "                        else:\n",
    "                            s+=1\n",
    "                \n",
    "                    for i in range(0,len(AA),1):\n",
    "                        if AA[i]==\"x\" or AA[i]==\"-\":\n",
    "                            i+=0\n",
    "                            start+=1\n",
    "                        else:\n",
    "                            p1=i\n",
    "                            for q in range(i,(len(AA)-s)+1,1):\n",
    "                                for j in range(q,len(AA),1):\n",
    "                                    if AA[j]==\"-\" and kmer==\"\":\n",
    "                                        j=q+1\n",
    "                                        p1=j\n",
    "                                        break\n",
    "                                    if AA[j]==\"-\":\n",
    "                                        j+=0\n",
    "                                    else:\n",
    "                                        p+=1\n",
    "                                        kmer+=AA[j]\n",
    "                                        if p==k:\n",
    "                                            p=0\n",
    "                                            p2=j\n",
    "                                            pos=(p1+1,p2+1)\n",
    "                                            #print(AA[p1:p2+1])\n",
    "                                            #print(kmer)\n",
    "                                            csv_writer1.writerow([kmer,pos,seqID,ai,subjectID,cloneID,sampleID])\n",
    "                                            j=q+1\n",
    "                                            p1=j\n",
    "                                            kmer=\"\"\n",
    "                                            break\n",
    "                            break   \n",
    "                            \n",
    "                \n",
    "                i=0\n",
    "                print(\"Kmers extraction starts ....\")     \n",
    "                with open(toFile1,'r') as csv_file:\n",
    "                    csv_reader = csv.DictReader(csv_file)\n",
    "                    with open(toFile2, 'w',newline='') as new_file1:\n",
    "                        csv_writer1 = csv.writer(new_file1)\n",
    "                        csv_writer1.writerow(['k-mer','position','seq_id','ai','subject_id','clone_id','sample_id'])\n",
    "                        for line in csv_reader:\n",
    "                            KmerS=(line['translatedSeq'])\n",
    "                            seqID=(line['seq_id'])\n",
    "                            ai =(line['ai'])\n",
    "                            cloneID=(line['clone_id'])\n",
    "                            sampleID=(line['sample_id'])\n",
    "                            subjectID=(line['subject_id'])\n",
    "                            #Function for the k-mers!\n",
    "                            kmersFunc(KmerS,20)\n",
    "                        print(\"Kmers extraction DONE!\")\n",
    "\n",
    "                mysql_conn.close_conn()\n",
    "    \n",
    "    \n",
    "    #######################################\n",
    "    # 2nd step of the preproccsing pipeline\n",
    "    def test_02_find_unique_by_score(self):\n",
    "        col_list = [\"k-mer\",\"ai\",\"clone_id\"]\n",
    "        DB = self.db_name #database name\n",
    "        dir_path = self.path_temp #temp file path\n",
    "\n",
    "\n",
    "        for subject_id in self.subjects:\n",
    "            output_file = dir_path + \"\\\\2_{}_{}_byScore.csv\".format(DB, subject_id)\n",
    "\n",
    "        if os.path.exists(output_file):\n",
    "            print(\"> Step No.2 of the preprocessing already done, continuing to step 3.\")\n",
    "\n",
    "        else:\n",
    "            print(\"ID = {}\".format(subject_id))\n",
    "            kmers=pd.read_csv(dir_path + r\"1_{}_{}_seqK.csv\".format(DB, subject_id), usecols=col_list)\n",
    "            \n",
    "            kmers = kmers.rename(columns = {'Unique-SeqKmer': 'Kmers'}, inplace = False)\n",
    "            kmers = kmers.rename(columns = {'k-mer': 'Kmers'}, inplace = False)\n",
    "            \n",
    "            \n",
    "            print(len(kmers))\n",
    "            kmers['id']=kmers.index\n",
    "            \n",
    "            l=kmers.values.tolist()\n",
    "            \n",
    "            d = {}\n",
    "            for i in tqdm(l):\n",
    "                d[i[0]] = []\n",
    "            for j in tqdm(l):\n",
    "                if (j[3] not in d[j[0]]):\n",
    "                    d[j[0]].append(j[3])\n",
    "            \n",
    "            \n",
    "            new_df=pd.DataFrame(kmers['Kmers'])\n",
    "            l=new_df.values.tolist()\n",
    "            flat_list = []\n",
    "            for sublist in l:\n",
    "                for item in sublist:\n",
    "                    flat_list.append(item)\n",
    "            l = flat_list\n",
    "            def count_uniqe(lst):\n",
    "                new_vals = Counter(l).most_common()\n",
    "                new_vals = new_vals[::1] #this sorts the list in scending order\n",
    "                return new_vals\n",
    "            \n",
    "            new_list=count_uniqe(l)\n",
    "            \n",
    "            df = pd.DataFrame(new_list, columns =['kmer', 'score'])\n",
    "\n",
    "            \n",
    "            def inx(kmer):\n",
    "                return d[kmer][0]\n",
    "            df['id']=df.kmer.apply(inx)\n",
    "            \n",
    "            df.to_csv(output_file, index=False)\n",
    "\n",
    "\n",
    "    ########################################\n",
    "    # 3rd step of the preprocessing pipeline\n",
    "    def test_03_reduce_using_varance_map(self):\n",
    "        pandarallel.initialize()\n",
    "        DB = self.db_name #database name\n",
    "        dir_path = self.path_temp #temp file path\n",
    "        \n",
    "        for subject_id in self.subjects:\n",
    "            input_file = dir_path + \"2_{}_{}_byScore.csv\".format(DB, subject_id)\n",
    "            output_file = dir_path + \"3_{}_{}_VarRemain.csv\".format(DB,subject_id)\n",
    "\n",
    "            if os.path.exists(output_file):\n",
    "                print(\"> Step No.3 of the preprocessing already done, continuing to step 4.\")\n",
    "\n",
    "            else:\n",
    "                print(\"ID = {}\".format(subject_id))\n",
    "\n",
    "                ########################\n",
    "                col_list = [\"kmer\",\"id\"]\n",
    "                col_list1 = [\"kmer\"]\n",
    "                kmers0=pd.read_csv(input_file, usecols=col_list)\n",
    "                kmers1=pd.read_csv(input_file, usecols=col_list1)\n",
    "                kmers=kmers1.copy()\n",
    "                kmers1['tmpid']=kmers1.index\n",
    "                kmers1['var']=0\n",
    "                \n",
    "                print(len(kmers0))\n",
    "                kmers0.head(5)\n",
    "                \n",
    "                ########################\n",
    "                kmers1['id']=kmers0['id']\n",
    "                \n",
    "                selected_columns = kmers1[[\"tmpid\",\"var\"]]\n",
    "                new_df = selected_columns.copy()\n",
    "                \n",
    "                all_list = new_df.values.tolist()\n",
    "                \n",
    "                All = {}\n",
    "                st = {}\n",
    "                for l in tqdm(all_list):\n",
    "                    All[l[0]] = l[1]\n",
    "                    st[l[0]] = l[1]\n",
    "                \n",
    "                #All\n",
    "                ########################\n",
    "                def diff_letters(a,b):\n",
    "                    cnt=0\n",
    "                    for i in range(len(a)):\n",
    "                        if a[i] != b[i] and a[i]!='x' and b[i]!='x':\n",
    "                            cnt+=1\n",
    "                            if cnt>1:\n",
    "                                break\n",
    "                    return cnt\n",
    "                \n",
    "                def wrap_f(x):\n",
    "                    def fiten(row):\n",
    "                        row = row[:10]\n",
    "                        return row\n",
    "                    return fiten(x['kmer'])\n",
    "                \n",
    "                def wrap_l(x):\n",
    "                    def laten(row):\n",
    "                        row = row[10:]\n",
    "                        return row\n",
    "                    return laten(x['kmer'])\n",
    "                \n",
    "                ########################\n",
    "                kmers['first']=kmers.parallel_apply(wrap_f,axis=1)\n",
    "                kmers['last']=kmers.parallel_apply(wrap_l,axis=1) \n",
    "                kmers['kmer']=kmers.index\n",
    "                kmers\n",
    "                \n",
    "                ########################\n",
    "                lastlist = kmers.values.tolist()\n",
    "                \n",
    "                removed = {}\n",
    "                for i in tqdm(lastlist):\n",
    "                    removed[i[0]] = []\n",
    "                \n",
    "                last = {}\n",
    "                for l in tqdm(lastlist):\n",
    "                    last[l[0]] = l[2]\n",
    "\n",
    "\n",
    "                first = {}\n",
    "                for l in tqdm(lastlist):\n",
    "                    first[l[0]] = l[1]\n",
    "                \n",
    "\n",
    "                firstd = {}\n",
    "                for i in tqdm(lastlist):\n",
    "                    firstd[i[1]] = []\n",
    "                \n",
    "                for j in tqdm(lastlist):\n",
    "                    firstd[j[1]].append(j[0])\n",
    "\n",
    "\n",
    "                lastd = {}\n",
    "                for i in tqdm(lastlist):\n",
    "                    lastd[i[2]] = []\n",
    "                \n",
    "                for j in tqdm(lastlist):\n",
    "                    lastd[j[2]].append(j[0])\n",
    "\n",
    "                ########################\n",
    "                def mapf():\n",
    "                    r=[]\n",
    "                    for l in tqdm(firstd.values()):\n",
    "                        if len(l)>1:\n",
    "                            for i in range(len(l)):\n",
    "                                if st[l[i]]!=0:\n",
    "                                    continue\n",
    "                                for j in range(i + 1, len(l)):\n",
    "                                    if st[l[j]]==0:\n",
    "                                        if diff_letters(last[l[i]],last[l[j]])<=1: \n",
    "                                            All[l[i]]+=1\n",
    "                                            st[l[j]]+=1\n",
    "                                            removed[l[i]].append(l[j])\n",
    "                                            r.append(l[j])\n",
    "                                \n",
    "                #second comparision\n",
    "                    for e in tqdm(lastd.values()):\n",
    "                        if len(e)>1:\n",
    "                            for x in range(len(e)):\n",
    "                                if st[e[x]]!=0:\n",
    "                                    continue\n",
    "                                for y in range(x + 1, len(e)):\n",
    "                                    if st[e[y]]==0:\n",
    "                                        if diff_letters(first[e[x]],first[e[y]])<=1: \n",
    "                                            All[e[x]]+=1\n",
    "                                            st[e[y]]+=1\n",
    "                                            removed[e[x]].append(e[y])\n",
    "                                            r.append(e[y])\n",
    "                    return r\n",
    "                \n",
    "                ########################\n",
    "                trash=mapf()\n",
    "                ss=list(set(trash))\n",
    "                print(\"number of kmers to be deleted: \\n{0:d}\".format(len(ss)))\n",
    "                \n",
    "                ########################\n",
    "                var=pd.DataFrame.from_dict(All, orient='index',columns=['variance'])\n",
    "                final = kmers1.drop(columns=['tmpid'])\n",
    "                final['var'] =var['variance']\n",
    "                final=final.sort_values(by=['var'],ascending=False)\n",
    "                final\n",
    "                \n",
    "                ########################\n",
    "                final = final.drop(trash)\n",
    "                print(\"the final number of kmers: \\n{0:d}\".format(len(final)))\n",
    "                final.head()\n",
    "\n",
    "                final.to_csv(output_file, index=False)\n",
    "\n",
    "\n",
    "    #########################################\n",
    "    # 4th step of the preprocessing  pipeline\n",
    "    def test_04_siliding_window(self):\n",
    "        DB = self.db_name #database name\n",
    "        dir_path = self.path_temp #temp file path\n",
    "        \n",
    "        for subject_id in self.subjects:\n",
    "            input_file = dir_path + \"3_{}_{}_VarRemain.csv\".format(DB, subject_id)\n",
    "            output_file = dir_path + \"4_{}_{}_slidingwindow_Var.csv\".format(DB, subject_id)\n",
    "\n",
    "            if os.path.exists(output_file):\n",
    "                print(\"> Step No.4 of the preprocessing already done, continuing to step 5.\")\n",
    "\n",
    "            else:\n",
    "                df = pd.read_csv(input_file, usecols=[\"kmer\"])\n",
    "                \n",
    "                #####\n",
    "                result=[]\n",
    "                index_tracker = 0\n",
    "                threshold = 0\n",
    "                total_rows = len(df)\n",
    "                \n",
    "                #####\n",
    "                for kmer in df['kmer']:\n",
    "                    if index_tracker > threshold + 1000000:\n",
    "                        print(\"On row {}/{}\".format(index_tracker, total_rows))\n",
    "                        threshold = index_tracker\n",
    "                    index_tracker += 1\n",
    "                \n",
    "                    SlidWindowStr = [kmer[i:i+3] for i in range(len(kmer)-2)]\n",
    "                    result.append(SlidWindowStr)\n",
    "                \n",
    "                df[\"SlidingWindow\"]=result    \n",
    "                \n",
    "                #####\n",
    "                df.to_csv(output_file, index=False)\n",
    "\n",
    "\n",
    "    ########################################\n",
    "    # 5th step of the preprocessing pipeline\n",
    "    def test_05_trimers_filter(self):\n",
    "        DB = self.db_name #database name\n",
    "        dir_path = self.path_temp #temp file path\n",
    "        \n",
    "        for subject_id in self.subjects:\n",
    "            input_file = dir_path + \"4_{}_{}_slidingwindow_Var.csv\".format(DB, subject_id)\n",
    "            output_file = dir_path + \"5_{}_{}_filtered_trimers_VarRemain.csv\".format(DB, subject_id)\n",
    "\n",
    "            if os.path.exists(output_file):\n",
    "                print(\"> Step No.5 of the preprocessing already done, continuing to step 6.\")\n",
    "\n",
    "            else:\n",
    "                print(\"ID = {}\".format(subject_id))\n",
    "    \n",
    "                # SlidingWindow kmers after Variance PATH\n",
    "                df = pd.read_csv(input_file)\n",
    "                \n",
    "                # Trimers dictionary PATH\n",
    "                vocab = pd.read_csv(self.trimer_dict_path, index_col=False)\n",
    "                \n",
    "                Trimer_dict = pd.Series(vocab.index,index=vocab.trimer).to_dict()\n",
    "                \n",
    "                trimers_of_interest = set(vocab['trimer'].tolist())\n",
    "                \n",
    "                flag=0\n",
    "                required_trimers=set()\n",
    "                not_required_trimers=set()\n",
    "                for index, row in tqdm(df.iterrows()):\n",
    "                    kmer_list = row['SlidingWindow']\n",
    "                    kmer_list = re.sub(r\"[^A-Za-z0-9(),]\", \"\", kmer_list)\n",
    "                    kmer_list = re.sub(r\"[^A-Za-z0-9()]\", \" \", kmer_list)\n",
    "                    kmer_list = list(kmer_list.split(\" \"))\n",
    "                    for kmer in kmer_list:\n",
    "                        if kmer in trimers_of_interest:\n",
    "                            required_trimers.add(kmer)\n",
    "                            flag=1\n",
    "                        else:\n",
    "                            not_required_trimers.add(kmer)\n",
    "                    if flag==0:\n",
    "                        print(index)\n",
    "                    flag=0\n",
    "                \n",
    "                # required_trimers_sorted\n",
    "                required_trimers_sorted=[]\n",
    "                for trimer in vocab['trimer']:\n",
    "                    if trimer in required_trimers:\n",
    "                        required_trimers_sorted.append(trimer)\n",
    "                \n",
    "                len(required_trimers_sorted)\n",
    "                \n",
    "                result=pd.DataFrame(data=required_trimers_sorted,columns=[\"trimer\"])\n",
    "                \n",
    "                # Filtered trimers save PATH\n",
    "                result.to_csv(output_file)\n",
    "\n",
    "\n",
    "    ########################################\n",
    "    # 6th step in the preprocessing pipeline\n",
    "    def test_06_sliding_window_filter(self):\n",
    "        DB = self.db_name #database name\n",
    "        dir_path = self.path_temp #temp file path\n",
    "        \n",
    "        for subject_id in self.subjects:\n",
    "            input_df = dir_path + \"4_{}_{}_slidingwindow_Var.csv\".format(DB, subject_id)\n",
    "            input_vocab = dir_path + \"5_{}_{}_filtered_trimers_VarRemain.csv\".format(DB, subject_id)\n",
    "            output_file = dir_path + \"6_{}_{}_svar_SlidingWindow_filter.csv\".format(DB, subject_id)\n",
    "\n",
    "            if os.path.exists(output_file):\n",
    "                print(\"> Step No.6 of the preprocessing already done, continuing to step 7.\")\n",
    "\n",
    "            else:\n",
    "                # SlidingWindow kmers after variance PATH\n",
    "                df = pd.read_csv(input_df, usecols=[\"SlidingWindow\"])\n",
    "                \n",
    "                vocab = pd.read_csv(input_vocab, index_col=False)\n",
    "                vocab.drop('Unnamed: 0',axis='columns', inplace=True)\n",
    "                \n",
    "                li=set(vocab['trimer'])\n",
    "                \n",
    "                reqiured_kmers=set()\n",
    "                for index, row in tqdm(df.iterrows()):\n",
    "                    kmer_list = row['SlidingWindow']\n",
    "                    kmer_list = re.sub(r\"[^A-Za-z0-9(),]\", \"\", kmer_list)\n",
    "                    kmer_list = re.sub(r\"[^A-Za-z0-9()]\", \" \", kmer_list)\n",
    "                    kmer_list = set(kmer_list.split(\" \"))\n",
    "                    for kmer in kmer_list:\n",
    "                        if kmer in li:\n",
    "                            reqiured_kmers.add(index)            \n",
    "                reqiured_kmers\n",
    "\n",
    "                \n",
    "                new_list=[]\n",
    "                for kmer in reqiured_kmers:\n",
    "                    new_list.append(df.iloc[kmer])\n",
    "                \n",
    "                df_new=pd.DataFrame(data=new_list)\n",
    "                df_new\n",
    "                \n",
    "                df_new.loc[df_new.index==349050]\n",
    "                \n",
    "                counter=0\n",
    "                for i in df_new.index:\n",
    "                    if i != counter:\n",
    "                        print(counter)\n",
    "                        counter+=1\n",
    "                    counter+=1\n",
    "                \n",
    "                # Filtered sliding window kmers PATH\n",
    "                df_new.to_csv(output_file, index=False)\n",
    "\n",
    "\n",
    "    ###################################################\n",
    "    # 7th (and last) step of the preprocessing pipeline\n",
    "    def test_07_finding_trimers_weights(self):\n",
    "        DB = self.db_name #database name\n",
    "        dir_path = self.path_temp #temp file path\n",
    "        \n",
    "        for subject_id in self.subjects:\n",
    "            input_df = dir_path + \"6_{}_{}_svar_SlidingWindow_filter.csv\".format(DB, subject_id)\n",
    "            output_file = dir_path + \"7_{}_{}_VarRemain_trimer_weights.p\".format(DB, subject_id)\n",
    "\n",
    "            # Trimers dictionary PATH\n",
    "            #Trimers Table    \n",
    "            TriMers = pd.read_csv(self.trimer_dict_path)\n",
    "\n",
    "            #Drop Index Column\n",
    "            TriMers.drop('index',axis='columns', inplace=True)\n",
    "            # Using DataFrame.insert() to add a column\n",
    "            TriMers[\"Places\"] = \"\"\n",
    "            TriMers\n",
    "\n",
    "            ### Convert trimer lookup into a dictionary\n",
    "            trimer_dict = {}\n",
    "            for index, row in TriMers.iterrows():\n",
    "                trimer = row['trimer']\n",
    "                trimer_dict[trimer] = index\n",
    "\n",
    "            trimer_dict\n",
    "\n",
    "            if os.path.exists(output_file):\n",
    "                print(\"> Step No.7 of the preprocessing already done, run pipeline 2 (matrix creation)\")\n",
    "\n",
    "            else:\n",
    "                print(\"ID = {}\".format(subject_id))\n",
    "                \n",
    "                # Filtered slidingwindow kmers PATH\n",
    "                #Read CSV File\n",
    "                df = pd.read_csv(input_df)\n",
    "                \n",
    "                ### Find trimer Weights\n",
    "                #### Make dictionary to save results\n",
    "                result = {}\n",
    "                for trimer in TriMers['trimer']:\n",
    "                    result[trimer] = 0\n",
    "                \n",
    "                trimers_of_interest = set(TriMers['trimer'].tolist())\n",
    "                for index, row in tqdm(df.iterrows()):\n",
    "                    kmer_list = row['SlidingWindow']\n",
    "                    kmer_list=eval(kmer_list)\n",
    "                    #print(type(kmer))\n",
    "                    for kmer in kmer_list:\n",
    "                        if kmer in trimers_of_interest:\n",
    "                            result[kmer]+=1\n",
    "                \n",
    "                ## Convert to weight   (IDF Equation)\n",
    "                total_seqs=len(df) \n",
    "                temp={}\n",
    "                for key,value in result.items():\n",
    "                    if value!=0:\n",
    "                        idf=math.log10(total_seqs/value)\n",
    "                        temp[key]=idf\n",
    "                result=temp\n",
    "                \n",
    "                # Trimers weights Save PATH\n",
    "                pickle.dump(result, open(output_file, \"wb\"))\n",
    "                                         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3be4f43c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> folder `reports\\covid_vaccine_new\\` exists, continuing.\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "issubclass() arg 1 must be a class",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 6\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Execute the function\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m----> 6\u001b[0m     pipeline1_result \u001b[38;5;241m=\u001b[39m \u001b[43mrun_pipeline\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpipeline1_preprocessing\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\github\\tms_preprocessing\\source\\helpers.py:145\u001b[0m, in \u001b[0;36mrun_pipeline\u001b[1;34m(test_pipeline, pipeline_name)\u001b[0m\n\u001b[0;32m    142\u001b[0m loader \u001b[38;5;241m=\u001b[39m unittest\u001b[38;5;241m.\u001b[39mTestLoader()\n\u001b[0;32m    144\u001b[0m \u001b[38;5;66;03m# Load tests from the specific class\u001b[39;00m\n\u001b[1;32m--> 145\u001b[0m suite \u001b[38;5;241m=\u001b[39m \u001b[43mloader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloadTestsFromTestCase\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtest_pipeline\u001b[49m\u001b[43m)\u001b[49m \n\u001b[0;32m    147\u001b[0m \u001b[38;5;66;03m# Run with high verbosity for detail\u001b[39;00m\n\u001b[0;32m    148\u001b[0m result \u001b[38;5;241m=\u001b[39m runner\u001b[38;5;241m.\u001b[39mrun(suite)\n",
      "File \u001b[1;32mc:\\Users\\Daniel\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\unittest\\loader.py:83\u001b[0m, in \u001b[0;36mTestLoader.loadTestsFromTestCase\u001b[1;34m(self, testCaseClass)\u001b[0m\n\u001b[0;32m     81\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mloadTestsFromTestCase\u001b[39m(\u001b[38;5;28mself\u001b[39m, testCaseClass):\n\u001b[0;32m     82\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Return a suite of all test cases contained in testCaseClass\"\"\"\u001b[39;00m\n\u001b[1;32m---> 83\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28;43missubclass\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtestCaseClass\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msuite\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTestSuite\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[0;32m     84\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTest cases should not be derived from \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     85\u001b[0m                         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTestSuite. Maybe you meant to derive from \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     86\u001b[0m                         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTestCase?\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     87\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m testCaseClass \u001b[38;5;129;01min\u001b[39;00m (case\u001b[38;5;241m.\u001b[39mTestCase, case\u001b[38;5;241m.\u001b[39mFunctionTestCase):\n\u001b[0;32m     88\u001b[0m         \u001b[38;5;66;03m# We don't load any tests from base types that should not be loaded.\u001b[39;00m\n",
      "\u001b[1;31mTypeError\u001b[0m: issubclass() arg 1 must be a class"
     ]
    }
   ],
   "source": [
    "from source.helpers import run_pipeline\n",
    "from source import pipeline1_preprocessing\n",
    "\n",
    "# Execute the function\n",
    "if __name__ == \"__main__\":\n",
    "    pipeline1_result = run_pipeline(pipeline1_preprocessing)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
